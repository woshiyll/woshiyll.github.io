<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<style type="text/css">
.styleTittle {
	FONT-SIZE: 45px; FONT-FAMILY: "Arial", Times, serif; 
}
.STYLEAuthor {
	FONT-SIZE: 20px; FONT-FAMILY: "Arial", Times, serif
}
.styleAffiliation {
	FONT-SIZE: 16px; FONT-FAMILY: "Arial", Times, serif
}
.STYLESection {
	FONT-SIZE: 30px; FONT-FAMILY: "Arial", Times, serif; FONT-WEIGHT: bold
}
.styleAbstract {
	FONT-SIZE: 20px; FONT-FAMILY: "Arial";
}
.STYLECaption {
	FONT-SIZE: 16px; FONT-FAMILY: "Arial"
}
.STYLECitation {
	FONT-SIZE: 17px; FONT-FAMILY: "Arial"
}
BODY {
	BACKGROUND-IMAGE: none
}
HR {
	BORDER-TOP: purple 1px solid; BORDER-RIGHT: purple 1px solid; BORDER-BOTTOM: purple 1px solid; BORDER-LEFT: purple 1px solid
}
SPAN.style231{
	FONT-FAMILY: "Times New Roman","serif"; mso-style-name: style231; mso-style-unhide: no; mso-ansi-font-size: 12.0pt; mso-bidi-font-size: 12.0pt; mso-ascii-font-family: "Times New Roman"; mso-hansi-font-family: "Times New Roman"; mso-bidi-font-family: "Times New Roman"
}
SPAN.style2311 {
	FONT-FAMILY: "Times New Roman","serif"; mso-style-name: style231; mso-style-unhide: no; mso-ansi-font-size: 12.0pt; mso-bidi-font-size: 12.0pt; mso-ascii-font-family: "Times New Roman"; mso-hansi-font-family: "Times New Roman"; mso-bidi-font-family: "Times New Roman"
}
SPAN.style23111 {
	FONT-FAMILY: "Times New Roman","serif"; mso-style-name: style231; mso-style-unhide: no; mso-ansi-font-size: 12.0pt; mso-bidi-font-size: 12.0pt; mso-ascii-font-family: "Times New Roman"; mso-hansi-font-family: "Times New Roman"; mso-bidi-font-family: "Times New Roman"
}
.style47 {
	FONT-SIZE: 16pt; COLOR: #000000
}
.style55 {
	FONT-SIZE: 16px; FONT-FAMILY: "Times New Roman", Times, serif; FONT-WEIGHT: bold
}
A:link {
	TEXT-DECORATION: none
}
A:visited {
	TEXT-DECORATION: none
}
A:hover {
	TEXT-DECORATION: none
}
A:active {
	TEXT-DECORATION: none
}
BODY {	
}

#container {
	WIDTH: 1024px; MARGIN: 0px auto;
}

</style>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>JCRNet
</title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/template.css" rel="stylesheet">
    <script type="text/javascript"><!-- 
    function obfuscate( domain, name ) { document.write('<a href="mai' + 
    'lto:' + name + '@' + domain + '">' + name + '@' + domain + '</' + 'a>'); }
    // --></script>
	
  </head>

<meta name="GENERATOR" content="MSHTML 11.00.10570.1001"></head>
<body onLoad=" ">
<div id="container" class="STYLE37">


<nav class="navbar navbar-inverse navbar-fixed-top" role="navigation" style="background:#7B7B7B">
        <!-- Brand and toggle get grouped for better mobile display -->
    <div class="container">
        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse navbar-ex1-collapse">
        <ul class="nav navbar-nav">
            <li><a href="index.html">Home</a></li>
            <li><a href="index.html#abstract">Abstract</a></li>
            <li><a href="index.html#framework">Framework</a></li>
            <li><a href="index.html#overview">Overview</a></li>
            <li><a href="index.html#example">Example</a></li>
            <li><a href="index.html#code">Code</a></li>
            <li><a href="index.html#citation">Citation</a></li>
            <li><a href="index.html#contact">Contact</a></li>
        </ul>
        </div>

    </div>
</nav>



<p class="styleTittle" align="center">Joint Correcting and Refinement for Balanced Low-Light Image Enhancement
</p>

<p align="center"> <font size = 4>IEEE Transactions on Multimedia</p>

<p align="center"><span class="STYLEAuthor"> 
    <a target="_blank">Nana Yu</a>, &nbsp;&nbsp;
    <a target="_blank">Hong Shi</a>, &nbsp;&nbsp;
    <a href="http://cic.tju.edu.cn/faculty/hanyahong/" target="_blank">Yahong Han</a>, <span style="font-style: oblique"></span> &nbsp;&nbsp;
   
    </span> 
    </p>

<p class="styleAffiliation" align="center">
    College of Intelligence and Computing, and Tianjin Key Lab of Machine Learning, Tianjin University<br>
<br>

<div class="container">
<a name="abstract"></a>
<p class="STYLESection" align="center">Abstract</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
<div class="project-page well" >
<p class="styleAbstract" align="justify"> Low-light image enhancement tasks demand an appropriate balance among brightness, color, and illumination. While existing methods often focus on one aspect of the image without considering how to pay attention to this balance, which will cause problems of color distortion and overexposure etc. This seriously affects both human visual perception and the performance of high-level visual models. In this work, a novel synergistic structure is proposed which can balance brightness, color, and illumination more effectively. Specifically, the proposed method, so-called Joint Correcting and Refinement Network (JCRNet), which mainly consists of three stages to balance brightness, color, and illumination of enhancement. Stage 1: we utilize a basic encoder-decoder and local supervision mechanism to extract local information and more comprehensive details for enhancement. Stage 2: cross-stage feature transmission and spatial feature transformation further facilitate color correction and feature refinement. Stage 3: we employ a dynamic illumination adjustment approach to embed residuals between predicted and ground truth images into the model, adaptively adjusting illumination balance. Extensive experiments demonstrate that the proposed method exhibits comprehensive performance advantages over 21 state-of-the-art methods on 9 benchmark datasets. Furthermore, a more persuasive experiment has been conducted to validate our approach the effectiveness in downstream visual tasks (e.g., saliency detection). Compared to several enhancement models, the proposed method effectively improves the segmentation results and quantitative metrics of saliency detection.</p>
</div>
<br><br>
</div>



<div class="container">
<a name="framework"></a>
<p class="STYLESection" align="center">Overall architecture of the Joint Correcting and Refinement Network</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./imgs/f-net.png" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify"> Fig. 1: Details of the JCRNet architecture. Specifically, the proposed JCRNet consists of three stages: feature extraction stage (FES), joint refinement stage (JRS), and illumination adjustment stage (IAS). The three stages work together to enhance low-light images, thereby balancing brightness, color, and exposure.
</p>
<br><br>
</div>



<div class="container">
<a name="overview"></a>
<p class="STYLESection" align="center">Quantitative Evaluation</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./imgs/f-index1.png" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify">Table. 1: Average performance comparison between multiple method and the proposed method enhancement on LOL, COCO and MIT datasets. OursPT is based on the PyTorch framework, while OursMS is based on the MindSpore framework. The best, second-best, and third-best scores are represented by red, blue, and green respectively.
</p>
<br><br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./imgs/f-index2.png" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify">Table. 2: Average performance comparison between multiple method and the proposed method enhancement on multiple no-reference dataset. OursPT is based on the PyTorch framework, while OursMS is based on the MindSpore framework. The best, second-best, and third-best scores are represented by red, blue, and green respectively. (T: Traditional methods. DL: Deep learning)
</p>
<br><br>
</div>

<div class="container">
<a name="example"></a>
<p class="STYLESection" align="center">Qualitative Evaluation</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./imgs/fig4.jpg" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify"> Fig. 2: Visualizing the results of the LOL dataset. For clarity, the magnified parts of the images are displayed.
</p>
<br><br>
</div>

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./imgs/fig5.jpg" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify"> Fig. 3: Visualizing the results of the COCO dataset. For a clear comparison of the visual effects, the error images between the ground-truth image and each enhancement result are shown in the lower left corner.
</p>
<br><br>
</div>

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./imgs/fig6.jpg" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify"> Fig. 4: Visualizing the results of the NPE dataset. For clarity, the magnified parts of the images are displayed.
</p>
<br><br>
</div>

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./imgs/fig7.jpg" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify"> Fig. 5: Visualizing the results of the VV dataset. For clarity, the magnified parts of the images are displayed.
</p>
<br><br>
</div>

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./imgs/fig8.jpg" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify"> Fig. 6: Visualizing the results of the Exdark dataset. For clarity, the magnified parts of the images are displayed.
</p>
<br><br>
</div>

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./imgs/fig9.jpg" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify"> Fig. 7: Visualizing the results of the MEF dataset. For clarity, the magnified parts of the images are displayed.
</p>
<br><br>
</div>


<div class="container">
<a name="code"></a>
<p class="STYLESection" align="center">Source Code</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">

<p class="STYLEAuthor">
<a href="https://github.com/woshiyll/JCRNet">JCRNet</a>


</p><br>
</div>


<div class="container">
<a name="citation"></a>
<p class="STYLESection" align="center">Citation</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
<p class="STYLECitation">
  <!-- @Article{CISA22,<br>
  author    = {Yucheng Shi, Yahong Han, Qinghua Hu, Yi Yang, and  Qi Tian},<br>
  title     = {Query-efficient Black-box Adversarial Attack with Customized Iteration and Sampling},<br>
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},<br>  
  year      = {2022},<br>
  doi       = {10.1109/TPAMI.2022.3169802}<br>
  }</p> 
<br> -->

<p class="lead">
    If you find this useful in your work, please consider citing the following reference:
    <div class="highlight">
    <pre> <code>@Article{JCRNet,
    author    = {Nana Yu, Hong Shi, Yahong Han},
    title     = {Joint Correcting and Refinement for Balanced Low-Light Image Enhancement},
    journal   = {IEEE Transaction on Multimedia},
    year      = {2023},
    doi       = {10.1109/TMM.2023.3348333}
    }
</code> </pre> 


</div>


<div class="container">
<a name="contact"></a>
<p class="STYLESection" align="center">Contact</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
</div>

<p class="STYLEAuthor">
Any question regarding this work can be addressed to 
<a href="yunana@tju.edu.cn">yunana@tju.edu.cn</a>.
</p
<br><br>


    
</pre></div>
</body></html>
